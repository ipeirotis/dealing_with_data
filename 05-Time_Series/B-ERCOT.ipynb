{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/dealing_with_data/blob/master/05-Time_Series/B-ERCOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x4Yt46mPB8R"
      },
      "source": [
        "# Capacity Planning for Texas's Power Grid with Time Series Analysis\n",
        "\n",
        "**The Business Problem:** The Electric Reliability Council of Texas (ERCOT) manages the power grid for most of Texas. A critical task is **capacity planning**: ensuring there is enough electricity to meet demand, especially during peaks, to avoid blackouts. Underestimating demand leads to power outages, while overestimating leads to wasted resources and higher costs.\n",
        "\n",
        "**Our Goal:** In this notebook, we'll act as consultants for ERCOT. We'll use time series analysis to decompose historical electricity demand data to understand its underlying patterns. This is the first step in building a predictive model for better capacity planning.\n",
        "\n",
        "ERCOT provides archival data on their grid, which we'll be using for our analysis. ERCOT releases the [electricity usage of their grid](http://www.ercot.com/gridinfo/load) and they [provide archival copies of their data](http://www.ercot.com/gridinfo/load/load_hist).\n",
        "(ERCOT also publishes their own load forecasts, this is a good baseline for any model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4igsapPPB8S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "\n",
        "!pip install -U -q statsmodels PyMySQL sqlalchemy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy import text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Setup\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# Change the graph defaults\n",
        "plt.rcParams['figure.figsize'] = (8, 3)  # Default figure size of 8x3 inches\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.color'] = 'lightgray'\n",
        "plt.rcParams['font.size'] = 10  # Default font size of 12 points\n",
        "plt.rcParams['lines.linewidth'] = 1  # Default line width of 1 points\n",
        "plt.rcParams['lines.markersize'] = 2  # Default marker size of 2 points\n",
        "plt.rcParams['legend.fontsize'] = 10  # Default legend font size of 10 points"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WKfMgG6cKlRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and plot the data\n"
      ],
      "metadata": {
        "id": "sMIfV993YAxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn_string = 'mysql+pymysql://{user}:{password}@{host}/{db}?charset=utf8mb4'.format(\n",
        "    host = 'db.ipeirotis.org',\n",
        "    user = 'student',\n",
        "    password = 'dwdstudent2025',\n",
        "    db = 'ercot',\n",
        "    encoding = 'utf8mb4')\n",
        "\n",
        "engine = create_engine(conn_string)\n",
        "\n",
        "# This query loads the dataset from the DB into the dataframe\n",
        "with engine.connect() as con:\n",
        "  sql = \"SELECT * FROM ercot.electricity\"\n",
        "  df = pd.read_sql(text(sql), con=con)\n",
        "  df = df.set_index('DATE_TIME')"
      ],
      "metadata": {
        "id": "EmH29w6f8yOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Q5S30e4P9NCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot(\n",
        "    logy=True,\n",
        "    title='ERCOT Consumption Data',\n",
        "    ylabel=\"Consumption\"\n",
        ")\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc='upper left') # move the legend out of the chart"
      ],
      "metadata": {
        "id": "iGDr3BZQ9KV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot(\n",
        "    y = 'ERCOT',\n",
        "    title='ERCOT Consumption Data',\n",
        "    ylabel=\"Consumption\"\n",
        ")"
      ],
      "metadata": {
        "id": "CLHTQf4AB-Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Mission: Answering Key Business Questions\n",
        "\n",
        "As we analyze this data, our goal is to answer the following questions for ERCOT's management:\n",
        "\n",
        "1.  **Demand Growth:** How is electricity demand evolving over time? Can we project the overall trend for the next 5 years for long-term capacity planning?\n",
        "2.  **Peak Demand:** We need to avoid blackouts. How can we project the *maximum* capacity needed? Can we create a confidence interval to show the range of maximum capacity required?\n",
        "3.  **Regional Analysis:** How do these patterns differ by region? This is crucial for regional capacity planning (e.g., for the COAST, WEST, etc.)."
      ],
      "metadata": {
        "id": "nde4siUcBAXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decomposing the Time Series\n",
        "\n",
        "To analyze this data effectively, we need to break it down into its core components. A time series is typically composed of:\n",
        "\n",
        "1.  **Trend (T):** The long-term direction of the data (e.g., is demand generally increasing?).\n",
        "2.  **Seasonality (S):** Predictable, repeating patterns or cycles (e.g., daily and yearly fluctuations).\n",
        "3.  **Residual (R):** The random, unpredictable noise left over after removing the trend and seasonality.\n",
        "\n",
        "We'll use a technique called **seasonality decomposition** to separate these components."
      ],
      "metadata": {
        "id": "O24z65d9VGja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We call our main time series Y\n",
        "Y = df['ERCOT']"
      ],
      "metadata": {
        "id": "enEJYjJ-BBy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Daily Seasonal Component"
      ],
      "metadata": {
        "id": "phLXeJ8hZFKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdL8ga6ZPB8n"
      },
      "outputs": [],
      "source": [
        "Y = df.ERCOT\n",
        "\n",
        "decompose = seasonal_decompose(Y,\n",
        "                                model='multiplicative',\n",
        "                                period=24,\n",
        "                                extrapolate_trend=24)\n",
        "\n",
        "T_d, S_d, R_d = decompose.trend, decompose.seasonal, decompose.resid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9hWcxGcPB8n"
      },
      "outputs": [],
      "source": [
        "# plot the trend, after we remove daily seasonality\n",
        "T_d.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of the Daily Seasonality\n",
        "\n",
        "The plot above shows the typical daily demand cycle. As expected, demand is lowest in the early morning hours and peaks in the late afternoon and evening as people come home from work and use more electricity. This daily pattern is a critical input for short-term operational planning."
      ],
      "metadata": {
        "id": "w0lznoLHV03U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMUTWiMOPB8n"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    S_d # the daily seasonal trend\n",
        "    .head(24 * 5) # plot the first five days\n",
        "    .plot()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also plot the residuals to see the quality of the removal of the seasonal component. Honestly, not a great outcome: our techniques are pretty naive, and do not account for the fluctuating magnitude of the changes (aka \"_clustered volatility_\"). Dealing with clustered volatility requires more advanced models than the ones we are currently using."
      ],
      "metadata": {
        "id": "7r9MkuNhQH5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    R_d # the residual factors after removing the daily seasonal\n",
        "    .head(24 * 30) # plot the first 30 days\n",
        "    .plot()\n",
        ")"
      ],
      "metadata": {
        "id": "pZTxQNE7t4lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing further the $T_d$ Component: Identifying Weekly Patterns\n",
        "\n",
        "We have extracted three time series (trend, seasonal, residual) from the main time series, after extracting the daily component. Now we will extract the weekly component, which has a duration of `period = 24 * 7` hours."
      ],
      "metadata": {
        "id": "fJIlFrTXY7cZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE1cIDUKPB8n"
      },
      "outputs": [],
      "source": [
        "decompose = seasonal_decompose(T_d,\n",
        "                                model='multiplicative',\n",
        "                                period=24 * 7,\n",
        "                                extrapolate_trend=24 * 7)\n",
        "\n",
        "T_w, S_w, R_w = decompose.trend, decompose.seasonal, decompose.resid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the remaining trend component after removing daily and weekly fluctuations\n",
        "T_w.plot()"
      ],
      "metadata": {
        "id": "Bv2rsbhddVIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of the Weekly Seasonality\n",
        "\n",
        "This plot reveals the weekly demand pattern. We can see a clear drop in demand during the weekends (Saturdays and Sundays) compared to the weekdays. This weekly pattern is essential for medium-term planning, such as scheduling maintenance."
      ],
      "metadata": {
        "id": "0l29-DX2WCBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1BMEMAPPB8n"
      },
      "outputs": [],
      "source": [
        "# This shows the weekly seasonality\n",
        "S_w.head(24*7).plot()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows the residual, after removing the daily and weekly\n",
        "R_w.head(24*365).plot()"
      ],
      "metadata": {
        "id": "SJgEf2d-vP7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing further the $T_w$ Component: Identifying yearly Patterns"
      ],
      "metadata": {
        "id": "QrF9KYSy_34s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decompose = seasonal_decompose(T_w,\n",
        "                                model='multiplicative',\n",
        "                                period=24 * 365,\n",
        "                                extrapolate_trend=24 * 365)\n",
        "\n",
        "T_y, S_y, R_y = decompose.trend, decompose.seasonal, decompose.resid"
      ],
      "metadata": {
        "id": "F28gloRwALgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows the overall trend, after removing daily, weekly, and yearly seasonality\n",
        "T_y.plot()"
      ],
      "metadata": {
        "id": "70jc1lAtAP_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows the yearly seasonality\n",
        "S_y.head(24*365).plot()"
      ],
      "metadata": {
        "id": "IgA4tMLUASgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uUk3GWDMMDqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "51bYgt2wL68n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows the overall trend, after removing daily, weekly, and yearly seasonality\n",
        "T_y.plot()"
      ],
      "metadata": {
        "id": "r5CKcp2ML3n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows the yearly seasonality. We show the\n",
        "# first 365 days * 24 hours as the pattern repeats in subsequent periods\n",
        "S_y.head(24*365).plot()"
      ],
      "metadata": {
        "id": "TFlIXb0hL_6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows the weekly seasonality. We show the first\n",
        "# 7 days multiplied with 24 hours as the pattern repeats in subsequent periods\n",
        "S_w.head(24*7).plot()"
      ],
      "metadata": {
        "id": "bcpEUpptMN4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows the daily seasonality. We show the first\n",
        "# 24 hours, as the pattern repeats in subsequent periods\n",
        "S_d.head(24).plot()"
      ],
      "metadata": {
        "id": "VdaLf_fvMH5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the time series with the overall trend, plus seasonality\n",
        "(T_y * S_y * S_w * S_d).plot()"
      ],
      "metadata": {
        "id": "BqUMlGlnnULI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the residual, that is not captured by the trend or seasonality\n",
        "# When we are modeling, we really talk about forecasting the trend and\n",
        "# potentially modeling this time series, which has been de-trended\n",
        "# and de-seasonalized.\n",
        "( R_y * R_w * R_d ).plot(figsize=(16,4), linewidth=0.5)"
      ],
      "metadata": {
        "id": "_WEcaZWqlpA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing the Residuals to Quantify Risk\n",
        "\n",
        "Now that we've isolated the Trend (T) and Seasonality (S), the final component to analyze is the **Residual (R)**. The residual represents the random, unpredictable \"noise\" in the data.\n",
        "\n",
        "From a business perspective, the residual is crucial for **risk management**. While we can't predict the exact noise, we can analyze its distribution to understand the potential range of random fluctuations. This helps us answer: \"How much of a safety buffer do we need for unexpected demand?\"\n",
        "\n",
        "Let's plot the residuals from our yearly decomposition."
      ],
      "metadata": {
        "id": "RBeqwEgGW3Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the histogram of the residuals.\n",
        "# Since these are multiplicative factors, it is a good idea\n",
        "# to also take the log and plot them again.\n",
        "( R_y * R_w * R_d ).hist(bins=1000)"
      ],
      "metadata": {
        "id": "-zsswGE1P_2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.log2( R_y * R_w * R_d ).hist(bins=1000)"
      ],
      "metadata": {
        "id": "ocXh2wA6UTkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrG5ap31Zqxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Quantiles to Set a Safety Buffer\n",
        "\n",
        "The histogram shows that most of the fluctuations are in the +/-25% range, and moatly centered around zero. However, there are \"tails\" representing larger, less frequent spikes. We can use **quantiles** to precisely measure these tails.\n",
        "\n",
        "This is how we can build a data-driven confidence interval for our planning."
      ],
      "metadata": {
        "id": "DPj4Z_txZrff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the quantiles of the residual distribution\n",
        "# These are the values with which we need to multiply our\n",
        "# trend and seasonality projections to estimate maximum capacity\n",
        "#\n",
        "# 99% = For 87.6 hours in a year, consumption is above this level\n",
        "# 99.9% = For 8.76 hours in a year, consumption is above this level\n",
        "# 99.99% = For 52 mins in a year, consumption is above this level\n",
        "# 99.999% = For 5.2 mins in a year, consumption is above this level\n",
        "\n",
        "q=[\n",
        "    0.00001, 0.0001,0.001,0.01,0.1,0.25,\n",
        "    0.5,\n",
        "    0.75,0.9,0.99,0.999,0.9999,0.99999\n",
        "]\n",
        "(R_y * R_w * R_d).quantile(q)"
      ],
      "metadata": {
        "id": "IYKxb4vycm9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots the histogram of the log of the time series after removing the trend\n",
        "# The division Y / T_y removes the long term trend from the series and returns\n",
        "# back the multiplicative factors S_y * S_w * S_d * R_y * R_w * R_d\n",
        "np.log(Y / T_y).hist(bins=1000, alpha=0.75)\n",
        "\n",
        "# Now let's remove the seasonal components as well and see the difference\n",
        "# Plots the histogram of the log of the residuals after removing trend and seasonality\n",
        "np.log(Y / (T_y * S_y * S_w * S_d)).hist(bins=1000, alpha=0.75)"
      ],
      "metadata": {
        "id": "L1AXjyiMkkdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion & Next Steps\n",
        "\n",
        "By decomposing the ERCOT time series, we've identified three key patterns:\n",
        "\n",
        "* **A consistent daily cycle** driven by typical work and life schedules.\n",
        "* **A clear weekly pattern** with lower demand on weekends and holidays.\n",
        "* **A strong yearly seasonality** with peaks in summer and winter.\n",
        "* **A clear upward trend** in overall electricity demand over the last two decades.\n",
        "\n",
        "**Next Steps:** With this understanding, a data science team could now move on to forecasting future demand. They would use the trend as a baseline and then layer the seasonal patterns on top to create a predictive model. This model would be the foundation for making informed decisions on capacity planning and resource allocation."
      ],
      "metadata": {
        "id": "OD9NLlfEW-6p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smR0EqQeCldO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix (Optional): Periodogram\n",
        "\n",
        "The periodogram is a graph that identifies the most important periodicities in the data. We typically apply that to a timeseries with zero trend. After that, we can extract the time periods to consider for periodicities. For most data, daily, weekly, and yearly are the three periods that we consider. In rare cases, a periodogram will reveal additional patterns."
      ],
      "metadata": {
        "id": "dg6r2QM6ClxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import periodogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create  a periodogram from the detrended ERCOT time series\n",
        "freqs, psd = periodogram(Y / T_y)\n",
        "\n",
        "# Create a dataframe with the results, and convert\n",
        "# frequencies to time between events by taking the inverse\n",
        "prd = pd.DataFrame({\"freqs\": freqs, \"psd\": psd})\n",
        "prd['period_hours'] = 1/prd['freqs']\n",
        "\n",
        "# Plot the results. Often people do a log on the y-axis, but\n",
        "# for identifying the major components, I think that linear works\n",
        "# better\n",
        "prd.plot(\n",
        "    x='period_hours',\n",
        "    y = 'psd',\n",
        "    ylabel = 'Power Spectral Density',\n",
        "    logx=True,\n",
        "    # logy=True\n",
        ")"
      ],
      "metadata": {
        "id": "P4IWyC24CnT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at what periods are the most powerful in the data\n",
        "agg_prd = prd.copy()\n",
        "\n",
        "agg_prd['period_hours'] = np.round(agg_prd['period_hours'])\n",
        "\n",
        "agg_prd = agg_prd.pivot_table(\n",
        "    index = 'period_hours',\n",
        "    values = 'psd',\n",
        "    aggfunc = 'sum'\n",
        ").reset_index()\n",
        "\n",
        "agg_prd.sort_values('psd', ascending=False).head(20)"
      ],
      "metadata": {
        "id": "QyXpORKsDHVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "ERCOT.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}