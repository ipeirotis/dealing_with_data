{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/dealing_with_data/blob/master/05-Time_Series/A-Introduction_to_Time_Series_using_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJXQvkmz9pmk"
      },
      "source": [
        "# Introduction to Time Series and Forecasting\n",
        "\n",
        "*Based on the book [Introduction to Time Series and Forecasting](https://link.springer.com/book/10.1007/978-3-319-29854-2) by Brockwell and Davis.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCYUqz2k9pmo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "\n",
        "!pip install -U -q PyMySQL sqlalchemy yfinance fredapi\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy import text\n",
        "\n",
        "from fredapi import Fred\n",
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Setup\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# Change the graph defaults\n",
        "plt.rcParams['figure.figsize'] = (8, 3)  # Default figure size of 6x2 inches\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.color'] = 'lightgray'\n",
        "plt.rcParams['font.size'] = 10  # Default font size of 12 points\n",
        "plt.rcParams['lines.linewidth'] = 1  # Default line width of 1 points\n",
        "plt.rcParams['lines.markersize'] = 3  # Default marker size of 3 points\n",
        "plt.rcParams['legend.fontsize'] = 10  # Default legend font size of 10 points"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0W0rt_1zaHEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Time Series?\n",
        "\n",
        "A time series is a set of observations $x_t$, each one being recorded at a specific time $t$. In our sessions, we focus on **_discrete_** time series, where observations are recorded at fixed time intervals (e.g., once an hour, or every 30 seconds, or every 7 days).\n",
        "\n",
        "We only consider **_regular_** time series, where we the time between observations is constant (i.e., we do not consider account deposits or withdrawals from an ATM that happen at various times; these are examples of an irregular time series)."
      ],
      "metadata": {
        "id": "D79hThX7_yOH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrymQoHL9pmn"
      },
      "source": [
        "## Examples of Time Series\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB301JbH9pmq"
      },
      "source": [
        "### Australian red wine \"sales\", (thousands of litres) monthly, Jan 80 - Oct 91"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSzhItBE9pmq"
      },
      "source": [
        "The file [`australian-wine-sales.txt`](https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt) contains the monthly sales of Australian red wines in for the period Jan-1980 to Oct-1991. Let's take a peak at the data file: We will use Pandas and the `pd.read_csv` function to read the text file into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_ybvQax9pmt"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt\"\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "##### What is this code?\n",
        "# The `read_csv` command can read directly from a URL, so we pass directly\n",
        "# the URL of the dataset as a parameter. Also, since the file uses the\n",
        "# tab character to separate the columns, we pass the `sep='\\t'` option to\n",
        "# the `read_csv` command, indicating that the separator is the \"tab\"\n",
        "# (i.e. `\\t` ) character.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDLr3O019pmt"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the time series:"
      ],
      "metadata": {
        "id": "gxVWM-gADlLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmOwjJ_b9pmu"
      },
      "outputs": [],
      "source": [
        "df.plot(\n",
        "    x = 'Date',\n",
        "    y = 'Sales'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VphKhVNF9pmw"
      },
      "source": [
        "It appears from the graph that the sales have an upward trend and a seasonal pattern with a peak in July and a trough in January."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical questions in Time Series Analysis\n",
        "\n",
        "* What is the overall trend?\n",
        "* Is the current month above or below expectations?\n",
        "* Do we observe any anomalies in our data?\n",
        "* What should we expect for next month? Next year?\n",
        "\n",
        "To answer such questions, we typically start by considering a few **models** of time series, and see how well such models capture the behavior of the data that we have.\n",
        "\n",
        "For example, it is important to recognize the presence of **seasonal components** and to remove them so as not to confuse them with long-term trends. This process is known as **seasonal adjustment**.\n",
        "\n"
      ],
      "metadata": {
        "id": "KI7CrL9Ub2ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Time Series"
      ],
      "metadata": {
        "id": "Azx22NzVZIJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NYC Accidents\n",
        "\n"
      ],
      "metadata": {
        "id": "nj953FOng8uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a look at the aggregate data for NYC accidents, we can start asking questions like:\n",
        "\n",
        "* Does the trend look good?\n",
        "* When do we observe spikes in accidents on a daily and weekly basis, so that we can deploy resources accordingly? (We can also do that on a geographical basis, but we will examine that separately)\n",
        "* Which dates or times are unusually high or low?"
      ],
      "metadata": {
        "id": "tzGae2-PnML6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "conn_string = 'mysql+pymysql://{user}:{password}@{host}/?charset=utf8mb4'.format(\n",
        "    host = 'db.ipeirotis.org',\n",
        "    user = 'student',\n",
        "    password = 'dwdstudent2015',\n",
        "    encoding = 'utf8mb4')\n",
        "mysql_engine = create_engine(conn_string)\n",
        "\n",
        "# Get the number of accidents per hour\n",
        "sql = '''\n",
        "  SELECT date_format(DATE_TIME,'%Y-%m-%d %H:00') AS acc_date, COUNT(*) AS accidents\n",
        "  FROM collisions.collisions\n",
        "  GROUP BY date_format(DATE_TIME,'%Y-%m-%d %H:00')\n",
        "  ORDER BY date_format(DATE_TIME,'%Y-%m-%d %H:00')\n",
        "'''\n",
        "\n",
        "# Read the results in Pandas\n",
        "with mysql_engine.connect() as conn:\n",
        "  acc_hourly = pd.read_sql(text(sql), con=conn)\n",
        "\n",
        "# Convert the acc_date column to datetime\n",
        "acc_hourly['acc_date'] = pd.to_datetime(acc_hourly['acc_date'])\n",
        "\n",
        "acc_hourly.plot(\n",
        "    kind='line',\n",
        "    x='acc_date',\n",
        "    y='accidents'\n",
        ")"
      ],
      "metadata": {
        "id": "o8J6P2kBZNLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can change the granularity of the time series by calculating aggregate statistics:"
      ],
      "metadata": {
        "id": "SsqyzJbjifGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### NYC Accidents Daily/Weekly/Monthly\n",
        "\n",
        "acc_aggregated = acc_hourly.copy()\n",
        "\n",
        "# When we want to perform resampling, we need the datetime to be the \"index\"\n",
        "acc_aggregated = acc_aggregated.set_index('acc_date')\n",
        "\n",
        "# Modify the aggregation here to get values for daily, weekly, monthly, etc\n",
        "acc_aggregated = acc_aggregated.resample('1W').sum()\n",
        "\n",
        "# Plot the aggregated data; by default, x-axis is the index\n",
        "acc_aggregated.plot(\n",
        "    kind='line',\n",
        "    y='accidents'\n",
        ")"
      ],
      "metadata": {
        "id": "Yx4DqcGMiSrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### US Population"
      ],
      "metadata": {
        "id": "tSxxbhu1pb_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the thousands=',' option to properly convert the population numbers to integers\n",
        "population = pd.read_csv(\"https://storage.googleapis.com/datasets_nyu/us-population.txt\", sep=' ', thousands=',')\n",
        "population[\"Year\"] = pd.to_numeric(population[\"Year\"])\n",
        "population[\"US_Population\"] = pd.to_numeric(population[\"US_Population\"])\n",
        "\n",
        "population.plot(\n",
        "    kind = 'line',\n",
        "    x = 'Year',\n",
        "    y = 'US_Population',\n",
        "    marker = 'o'\n",
        ")"
      ],
      "metadata": {
        "id": "5tyB5wfzkU_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stock prices\n",
        "\n",
        "If you are interested in downloading stock information, [this article contains a good discussion](https://towardsdatascience.com/a-comprehensive-guide-to-downloading-stock-prices-in-python-2cd93ff821d4)."
      ],
      "metadata": {
        "id": "HM6vYcs_pZuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df = yf.download(tickers = ['GOOG','AAPL','MSFT', 'IBM'],\n",
        "                       interval = '1d', # download daily prices\n",
        "                       start='2005-01-01', # fetch prices after 2004\n",
        "                       auto_adjust = True, # adjust for splits etc\n",
        "                       progress = False # do not show a progress bar\n",
        "                       )['Close'] # Keep only the closing price\n",
        "stock_df"
      ],
      "metadata": {
        "id": "Y17fnaQJo6Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.plot(\n",
        "    title=\"Daily Closing price\",\n",
        "    logy=True\n",
        ")"
      ],
      "metadata": {
        "id": "lfALYjjkpO3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variation of the plot, where we normalize all stocks\n",
        "# by dividing the stock with the price on the first day\n",
        "# of 2005.\n",
        "normalized_stock_df = stock_df.copy()\n",
        "first_date = normalized_stock_df.index[0]\n",
        "print(\"First date is\", first_date)\n",
        "\n",
        "first_entry = normalized_stock_df.loc[first_date]\n",
        "normalized_stock_df = normalized_stock_df / first_entry\n",
        "\n",
        "normalized_stock_df.plot(\n",
        "    title=\"Normalized Price (y=1 is at Jan 1 2005)\",\n",
        "    logy=True\n",
        ")"
      ],
      "metadata": {
        "id": "YQptrr7YstkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_stock_df.index[0]"
      ],
      "metadata": {
        "id": "_9zhuozrs8nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily returns\n",
        "daily_returns = stock_df.pct_change(1)\n",
        "daily_returns"
      ],
      "metadata": {
        "id": "DUbhXLpEhRnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_returns.describe()"
      ],
      "metadata": {
        "id": "KOS5YRR4VXPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_returns.plot(\n",
        "    y = 'AAPL'\n",
        ")"
      ],
      "metadata": {
        "id": "COW-X-XcrN3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_returns['AAPL'].hist(bins=100, color='Green', alpha=1, density=True, range=(-0.1,0.1))\n",
        "daily_returns['IBM'].hist(bins=100, color='Red', alpha=0.75, density=True, range=(-0.1,0.1))\n",
        "\n",
        "# If you want to add the KDE lines, uncomment the lines below\n",
        "# daily_returns['AAPL'].plot.kde(color='Green', alpha=1, linewidth=2, xlim=(-0.1,0.1))\n",
        "# daily_returns['IBM'].plot.kde(color='Red', alpha=1, linewidth=2, xlim=(-0.1,0.1))\n"
      ],
      "metadata": {
        "id": "e77_AnIiUlMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOBkZKW-YpV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise\n",
        "\n",
        "* Create a plot of the stock price for a company of your choice.\n",
        "* Bechmark the stock price on Jan 1, 2020 and see the performance before and after. (Hints: (a) you can use the command .loc['1976-05-03'] to get the value of a series on a particular date; (b) there is no trading on weekends and on holidays)\n",
        "* Calculate the returns of the stock on a daily basis.\n",
        "* How would you calculate the return on a coarser time granularity (e.g., weekly, monthly, etc)?"
      ],
      "metadata": {
        "id": "Q1P8N_7nauBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "cSsLNOMnbUyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A more involved exercise\n",
        "\n",
        "Here are several sector ETFs representing different areas of the economy. The ETFs below cover the 11 sectors of the Global Industry Classification Standard (GICS).\n",
        "\n",
        "* Energy: XLE (Energy Select Sector SPDR Fund). Includes oil, gas, and renewable energy companies\n",
        "* Financials: XLF (Financial Select Sector SPDR Fund). Represents banks, insurance companies, and other financial services firms\n",
        "* Healthcare: XLV (Health Care Select Sector SPDR Fund). Covers pharmaceutical companies, medical device manufacturers, and healthcare providers\n",
        "* Consumer Staples: XLP (Consumer Staples Select Sector SPDR Fund). Includes companies producing essential consumer goods like food, beverages, and household items\n",
        "* Consumer Discretionary: XLY (Consumer Discretionary Select Sector SPDR Fund). Represents companies producing non-essential goods and services, like retail, automotive, and entertainment\n",
        "* Materials: XLB (Materials Select Sector SPDR Fund). Covers companies involved in the production of raw materials, chemicals, and construction materials\n",
        "* Real Estate: XLRE (Real Estate Select Sector SPDR Fund). Includes REITs and other real estate companies\n",
        "* Industrials: XLI (Industrial Select Sector SPDR Fund). Represents manufacturing, aerospace, defense, and transportation companies\n",
        "* Communication Services: XLC (Communication Services Select Sector SPDR Fund) Includes telecommunication, media, and entertainment companies\n",
        "* Utilities: XLU (Utilities Select Sector SPDR Fund)\n",
        "* Tech: XLK (Tech Select Sector SPDR Fund). Covers technology companies\n",
        "\n",
        "***Tasks***\n",
        "\n",
        "* Download the ETF prices since Jan 1, 2000.\n",
        "* Normalize their prices and plot how the ETF prices have evolved since 2000.\n",
        "* Calculate the daily returns of the ETFs.\n",
        "* Use the `.describe()` command to calculate the risk profile and the returns of each sector. Which sector has performed best, which one has performed the worse?\n",
        "* Use the `.corr()` to calculate the degree of correlations of the returns across sectors.\n",
        "* Use the `seaborn.heatmap` to visualize the correlation matric across sectors.\n",
        "* Use the `seaborn.pairplot(... kind='reg')` to create pairwise scatterplots that show the correlation of the returns using scatterplots. Here is a command that makes the regression line red, and makes the scatterplots more transparent.\n",
        "`sns.pairplot(returns, kind='reg', plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.01}})`\n"
      ],
      "metadata": {
        "id": "oKTGbBYRaa0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "etf_tickers = ['XLK', 'XLU', 'XLE', 'XLF', 'XLV', 'XLP', 'XLY', 'XLB', 'XLRE', 'XLI', 'XLC']\n",
        "\n",
        "etf_names = ['Tech', 'Utilities', 'Energy', 'Financials', 'Healthcare', 'Consumer Staples', 'Consumer Discretionary', 'Materials', 'Real Estate', 'Industrials', 'Communication Services']"
      ],
      "metadata": {
        "id": "T4AuoaoFhCta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.pairplot(returns_etf, kind='reg', plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.01}})\n"
      ],
      "metadata": {
        "id": "YGWlSjALZ3kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FRED Economic Data\n",
        "\n",
        "The [FRED Economic Data by the Federal Reserve Bank of St Louis](https://fred.stlouisfed.org/) publishes a rich set of 822,000 US and international time series from 110 sources. Here are a few examples:"
      ],
      "metadata": {
        "id": "DycG4d3XiOPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fred = Fred(api_key='c041995ed8b9ab9c3f475e2ca8f7c288')"
      ],
      "metadata": {
        "id": "iPcAIhXMpOfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consumer Price Index for All Urban Consumers, not seasonally-adjusted\n",
        "# https://fred.stlouisfed.org/series/CPIAUCNS\n",
        "\n",
        "cpi = fred.get_series('CPIAUCNS')"
      ],
      "metadata": {
        "id": "3p6s4XHfiQ7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpi"
      ],
      "metadata": {
        "id": "gw6ntWJrpHrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpi.plot()"
      ],
      "metadata": {
        "id": "-a3UKfeOjk7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CPI data is monthly. The code below\n",
        "# calculates the 12-month percentage change\n",
        "cpi.pct_change(12).plot()"
      ],
      "metadata": {
        "id": "vTT5vCZzjePH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVZ3_B-dqpb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization of time series (Exercise)\n",
        "\n",
        "* Retrieve and plot the [Advance Retail Sales: Retail Trade and Food Services](https://fred.stlouisfed.org/series/RSAFSNA)\n",
        "* Normalize the values, to account for inflation. Use the CPI time series."
      ],
      "metadata": {
        "id": "DfUpL4-wqqpA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEJnifFqqiDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Retrieve and plot the S&P 500 index from Yahoo Finance. Its code is `'^GSPC'`; use the closing price for each date.\n",
        "* To evaluate its performance in the 21st century, divide the time series with its value in January 2000. (Hints: (a) you can use the command `.loc['1976-05-03']` to get the value of a series on a particular date; (b) there is no trading on weekends and on holidays)\n",
        "* Use the CPI values to examine the performance of the S&P500 after accounting for inflation. (Hint: Notice that the time series have different temporal granularity; think how to make them comparable)"
      ],
      "metadata": {
        "id": "gXPvqBvPvpEA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UcTIjqYDY_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Time Series: Autocorrelation"
      ],
      "metadata": {
        "id": "5nysr3TywnJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A commonly analyzed property of a time series is the **autocorrelation** of the sequential observations. Simply stated, a high degree of autocorrelation means that if we know the value at time $t$, we can predict well the value at $t+1$.\n",
        "\n",
        "Using the `autocorr` function, we estimate the autocorrelation of our `Sales` time series:"
      ],
      "metadata": {
        "id": "PVV7Wv-gwwkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the wine sales data\n",
        "url = \"https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt\"\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Converting the string date to proper datetime format\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.set_index(\"Date\")\n",
        "# Ensuring that sales is a numeric variable\n",
        "df[\"Sales\"] = pd.to_numeric(df[\"Sales\"])\n"
      ],
      "metadata": {
        "id": "fAj9dgsiahlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sales\"].autocorr()"
      ],
      "metadata": {
        "id": "TpIx6XNQxBbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of autocorrelation can extend to longer time periods, and not just to $t$ and $t+1$. We can extract autocorrelation for various **lag** values."
      ],
      "metadata": {
        "id": "o1S-QwxLxgUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sales\"].autocorr(lag=1) # same as simply df[\"Sales\"].autocorr()"
      ],
      "metadata": {
        "id": "RG_8UkVpx7bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between t and t+2\n",
        "# ie sales now and 2 months later\n",
        "df[\"Sales\"].autocorr(lag=2)"
      ],
      "metadata": {
        "id": "F0eYRnNExBer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between t and t+3\n",
        "# ie sales now and 3 months later\n",
        "df[\"Sales\"].autocorr(lag=3)"
      ],
      "metadata": {
        "id": "voqmEUsbIXwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNpq-WQk9pmw"
      },
      "source": [
        "### Lag plots and  autocorrelation plots\n",
        "\n",
        "Pandas provides two types of plots that can be used for the analysis of time series: the `lag_plot` and the `autocorrelation_plot`. We can also use the seasonal decomposition functionality of `statsmodels` to separate the time series into a trend, seasonal component, and residual noise. We will go quickly over these for now, mainly for demo purposes. Proper treatment of these topics require deeper analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX8gIodj9pmw"
      },
      "source": [
        "#### Lag plot\n",
        "\n",
        "By default, the lag plot shows the value of the series at time $t$ vs. its value at time $t+1$. If there is no dependency (i.e., the time series is noise) then the lag plot is a scatterplot without any sign of correlation. If we can see a pattern and a correlation, then the series exhibits autocorrelation. For example, below we can see that there is a rather strong correlation of the two variables, indicating that the sales in time $t+1$ is similar to the sales at time $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_U1MAtJ9pmw"
      },
      "outputs": [],
      "source": [
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 1, c='b')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The plot above shows the autocorrelation of the t and t+1\n",
        "# is around 0.73\n",
        "df[\"Sales\"].autocorr(lag=1)"
      ],
      "metadata": {
        "id": "yo3Mbmp2zCFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the lag plot, where we plot $t$ and $t+12$. Notice that we have a higher correlation (less spread out points)"
      ],
      "metadata": {
        "id": "v8hotHdoyq5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 12, c='r')"
      ],
      "metadata": {
        "id": "cSpqApSYyofW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sales\"].autocorr(lag=12)"
      ],
      "metadata": {
        "id": "5DABBWdSy5cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the two of them together."
      ],
      "metadata": {
        "id": "Ygo-Me32zV1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 1, c='b')\n",
        "pd.plotting.lag_plot(df[\"Sales\"], lag = 12, c='r')"
      ],
      "metadata": {
        "id": "Sw6dQj20zRXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axNcEjbR9pmx"
      },
      "source": [
        "#### Autocorrelation Plot\n",
        "\n",
        "In a more general setting, we want to also see if the value of the series at time $t$ is predictive of the value at time $t+n$. Such dependency would indicate that there is *autocorrelation* in the series. The autocorrelation plot shows the correlation value for various values of $n$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmk93mL9pmy"
      },
      "outputs": [],
      "source": [
        "pd.plotting.autocorrelation_plot(df[\"Sales\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above, with the oscillating autocorrelation values indicate that there is a **seasonality** component in the time series. (As we see that the correlation in 12-month increments to go up and then down.)\n",
        "\n",
        "Let's see next how we can extract the seasonal component."
      ],
      "metadata": {
        "id": "5GIfjYKWoPqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Analyze the autocorrelation of the following series:\n",
        "\n",
        "* NYC Accidents (`acc_hourly` and `acc_aggregated` (weekly) data frames)\n",
        "\n",
        "* Stock prices (`normalized_stock_df` and `daily_returns`)\n",
        "\n",
        "* Monthly CPI data from FRED (`cpi`)"
      ],
      "metadata": {
        "id": "dB2ezT0Jgl2y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NaK3sesTr_8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmir3VtH9pmy"
      },
      "source": [
        "## Trend and Seasonal Decomposition\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In time series analysis, trend and seasonal decomposition are crucial to understand underlying patterns and recurring variations in your data. These methodologies enable better forecasting and decision-making, equipping you with a robust toolkit to leverage temporal data for business strategy. Let's dive deeper into how these methods can help us to effectively dissect and interpret our time series data."
      ],
      "metadata": {
        "id": "xveZC_aLwLUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGClDRKA9pmz"
      },
      "outputs": [],
      "source": [
        "# A simple technique for decomposition.\n",
        "# https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\n",
        "# Uses moving averages to calculate the trend\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# We decompose assumming a 12-month periodicity.\n",
        "# We can also specify a multiplicative instead of an additive model\n",
        "# The additive model is Y[t] = T[t] + S[t] + e[t]\n",
        "# The multiplicative model is Y[t] = T[t] * S[t] * e[t]\n",
        "decomposition = seasonal_decompose(\n",
        "    x = df['Sales'],\n",
        "    model='multiplicative',\n",
        "    period=12,\n",
        "    extrapolate_trend=24\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (10,8))\n",
        "fig = decomposition.plot(\n",
        "    observed=False,\n",
        "    seasonal=True,\n",
        "    trend=True,\n",
        "    resid=True,\n",
        ")"
      ],
      "metadata": {
        "id": "yDcFdz7wtk1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxIe9Gjc9pmz"
      },
      "source": [
        "#### Accessing indinvidual components of the decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jA9GC049pm0"
      },
      "source": [
        "Once we have the decomposed time series model, we can also access the different components.\n",
        "\n",
        "For example, we can get the trend of the time series, after removing the seasonality component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHhcWhrM9pm0"
      },
      "outputs": [],
      "source": [
        "# The outcome is a pandas Series, which is effectively the same as a single column of dataframe\n",
        "decomposition.trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr8pLOvd9pm0"
      },
      "outputs": [],
      "source": [
        "decomposition.trend.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Periodogram (optional, advanced)\n",
        "\n",
        "The periodogram is a graph that identifies the most important periodicities in the data. **We typically apply that to a timeseries with zero trend**. After that, we can extract the time periods to consider for periodicities. For most data, daily, weekly, and yearly are the three periods that we consider. In rare cases, a periodogram will reveal additional patterns."
      ],
      "metadata": {
        "id": "zGFzQqFCZF26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import periodogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create  a periodogram from the detrended sales time series\n",
        "timeseries = df[\"Sales\"] / decomposition.trend\n",
        "freqs, psd = periodogram(timeseries)\n",
        "\n",
        "# Create a dataframe with the results, and convert\n",
        "# frequencies to time between events (periods)\n",
        "# Remember that frequency = time per unit time (eg \"2 times per month\")\n",
        "# and period is the inverse (e.g., \"every half month\")\n",
        "prd = pd.DataFrame({\"freqs\": freqs, \"psd\": psd})\n",
        "prd['period'] = 1/prd['freqs']\n",
        "\n",
        "# Plot the results. Often people do a log on the y-axis, but\n",
        "# for identifying the major components, I think that linear works\n",
        "# better\n",
        "prd.plot(\n",
        "    x='period',\n",
        "    y = 'psd',\n",
        "    ylabel = 'Power Spectral Density',\n",
        "    logx=True,\n",
        "    # logy=True\n",
        ")"
      ],
      "metadata": {
        "id": "0jvGBedbZHft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo8bJ5RN9pm5"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "* Below we fetch the daily number of vehicular accidents in NYC.\n",
        "* Examine the autocorrelation structure of the accidents.\n",
        "* Perform a decomposition of the time series into a trend, seasonal, and residual component.\n",
        "* Try out both the additive and the multiplicative approach for the decomposition. Try to interpret and understand the difference in the reported seasonal component.\n",
        "* Instead of counting accidents, extract the number of injuries and perform the same analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conn_string = 'mysql+pymysql://{user}:{password}@{host}/{db}?charset=utf8mb4'.format(\n",
        "    host = 'db.ipeirotis.org',\n",
        "    user = 'student',\n",
        "    password = 'dwdstudent2015',\n",
        "    db = 'collisions',\n",
        "    encoding = 'utf8mb4')\n",
        "\n",
        "mysql_engine = create_engine(conn_string)\n",
        "\n",
        "sql = '''\n",
        "  SELECT date_format(DATE_TIME,'%Y-%m-%d') AS acc_date, COUNT(*) AS accidents\n",
        "  FROM collisions.collisions\n",
        "  GROUP BY date_format(DATE_TIME,'%Y-%m-%d')\n",
        "  ORDER BY date_format(DATE_TIME,'%Y-%m-%d')\n",
        "'''\n",
        "\n",
        "with mysql_engine.connect() as conn:\n",
        "  acc = pd.read_sql(text(sql), con=conn)\n",
        "acc['acc_date'] = pd.to_datetime(acc['acc_date'])\n",
        "acc = acc.set_index('acc_date')"
      ],
      "metadata": {
        "id": "rpVw1x6dEv0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc.plot()"
      ],
      "metadata": {
        "id": "sVEIpRawHxCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the autocorrelation"
      ],
      "metadata": {
        "id": "8osa3JEo8ogY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "52PxGdwTKoYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc['accidents'].autocorr()"
      ],
      "metadata": {
        "id": "FoCIOTrL6DmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['accidents'].autocorr(lag=2)"
      ],
      "metadata": {
        "id": "mIsfZu-J6KKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['accidents'].autocorr(lag=7)"
      ],
      "metadata": {
        "id": "FVWkbjvuvnz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['accidents'].autocorr(lag=365)"
      ],
      "metadata": {
        "id": "SZ9SjwZf6lkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.lag_plot(acc['accidents'], lag = 1, c='b', s=1)"
      ],
      "metadata": {
        "id": "lwxJZPj06V30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.lag_plot(acc['accidents'], lag = 7, c='b', s=1)"
      ],
      "metadata": {
        "id": "2u50u9nz6y5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.autocorrelation_plot(acc['accidents'])"
      ],
      "metadata": {
        "id": "EpUO5rKX6OhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot = pd.plotting.autocorrelation_plot(acc['accidents'])\n",
        "plot.set_xlim(0,90)"
      ],
      "metadata": {
        "id": "7NAsm40Iz4PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# We decompose assumming a 12-month periodicity.\n",
        "# We can also specify a multiplicative instead of an additive model\n",
        "# The additive model is Y[t] = T[t] + S[t] + e[t]\n",
        "# The multiplicative model is Y[t] = T[t] * S[t] * e[t]\n",
        "\n",
        "period = 7 # We have daily observations, and we consider one week\n",
        "             # as the seasonality period\n",
        "decomposition = seasonal_decompose(x = acc['accidents'],\n",
        "                                   model='multiplicative',\n",
        "                                   period=period,\n",
        "                                   extrapolate_trend=period\n",
        "                )\n",
        "\n",
        "fig = plt.figure(figsize = (10,8))\n",
        "fig = decomposition.plot(\n",
        "    observed=False,\n",
        "    seasonal=True,\n",
        "    trend=True,\n",
        "    resid=True,\n",
        ")"
      ],
      "metadata": {
        "id": "FWuEiHotJhuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the trend\n",
        "decomposition.trend.plot()"
      ],
      "metadata": {
        "id": "gyQqunWlm6NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the seasonal component for the first week of the data\n",
        "decomposition.seasonal.head(period).plot(figsize=(10,3))"
      ],
      "metadata": {
        "id": "YbaLAhL8J52l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the residual\n",
        "decomposition.resid.plot(figsize=(10,3))"
      ],
      "metadata": {
        "id": "ED086sc94wLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the dates with the highest/lowest\n",
        "# residual  components\n",
        "decomposition.resid.sort_values()"
      ],
      "metadata": {
        "id": "Nrf3TQ345LYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decomposition.resid.sort_values().head(10)"
      ],
      "metadata": {
        "id": "14SIGr2D67ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decomposition.resid.sort_values().tail(10)"
      ],
      "metadata": {
        "id": "w--ZsuiS6_qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the autocorrelation structure of the residuals\n",
        "ax = pd.plotting.autocorrelation_plot(decomposition.resid, linewidth=1)\n",
        "ax.set_xlim( [0,365] )\n",
        "pass"
      ],
      "metadata": {
        "id": "m5aNoYAh86Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly8PGn_k9pm6"
      },
      "source": [
        "## Advanced: Time Series Window operations: Rolling / Expanding / EW\n",
        "\n",
        "One question that comes up when we have a periodic time series is: \"How can I figure out the overall trend?\". In the examples above, we relied on a \"black box\" where we simply asked for the time series to be decomposed into a trend, seasonal, and residual component. Now, let's dig a bit deeper on how we can extract trend components that are unaffected by seasonality.\n",
        "\n",
        "For that, we often rely on \"window\" functions, that operate over a set of continuous time series points. For example, if we have a time series that has a 12-month seasonality, we can take the 12-month average, which will not exhibit seasonality, but will capture the trend.\n",
        "\n",
        "These windows functions are common time series operations. Pandas provides support for various types of windows. Here are a few that are commonly used:\n",
        "* [Rolling window](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html): We compute the function over a time period equal to a window\n",
        "* [Expanding](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.expanding.html): We compute the function over a period of 1, 2, 3,... instances.\n",
        "* [Exponential weighting](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html): We keep a window of a fixed size but we weight less and less (exponentially) the old data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqelKNEv9pm7"
      },
      "outputs": [],
      "source": [
        "# Use different linestyle, and use high alpha to make the series less visually prominent\n",
        "df['Sales'].plot(label='Raw', linestyle =\"--\", alpha=0.25)\n",
        "\n",
        "# Plot the 12-month moving average\n",
        "df['Sales'].rolling(12).mean().plot(label='12M MA', alpha=0.75)\n",
        "\n",
        "# Plot the expanding mean. This is the mean of the series from the beginning till that point in time\n",
        "df['Sales'].expanding().mean().plot(label='Expanding', alpha=0.75)\n",
        "\n",
        "# Plot the exponentially weighted moving average. This moving average weighs more heavily the newer\n",
        "# data points and weighs less the old ones.\n",
        "df['Sales'].ewm(halflife=12).mean().plot(label='EWMA (halflife 12M)', alpha=0.75)\n",
        "\n",
        "# places the legend to the right side (1) and middle of the y-axis (0.5)\n",
        "plt.legend(bbox_to_anchor=(1, .5))\n",
        "plt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "A- Introduction to Time Series and Forecasting",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}