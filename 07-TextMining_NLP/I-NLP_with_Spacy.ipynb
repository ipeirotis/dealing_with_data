{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the instructions from https://spacy.io/docs/usage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "Requirement already up-to-date: six in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: msgpack-numpy in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: dill<0.3,>=0.2 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: cymem<1.32,>=1.30 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: numpy>=1.7 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: msgpack-python in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: pathlib in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: ujson>=1.35 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: regex==2017.4.5 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.5/dist-packages (from spacy)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.5/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: termcolor in /usr/local/lib/python3.5/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.5/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: wrapt in /usr/local/lib/python3.5/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: wcwidth in /usr/local/lib/python3.5/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already up-to-date: html5lib in /usr/local/lib/python3.5/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already up-to-date: toolz>=0.8.0 in /usr/local/lib/python3.5/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "Collecting setuptools>=18.5 (from html5lib->ftfy<5.0.0,>=4.4.2->spacy)\n",
      "  Downloading setuptools-37.0.0-py2.py3-none-any.whl (481kB)\n",
      "\u001b[K    100% |████████████████████████████████| 491kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: webencodings in /usr/local/lib/python3.5/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Installing collected packages: spacy, setuptools\n",
      "  Found existing installation: setuptools 36.7.2\n",
      "    Uninstalling setuptools-36.7.2:\n",
      "      Successfully uninstalled setuptools-36.7.2\n",
      "Successfully installed setuptools-37.0.0 spacy-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!sudo -H python3 -m pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Models for various languages\n",
    "\n",
    "See https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 852.3MB 86.0MB/s ta 0:00:01    11% |███▊                            | 100.3MB 87.4MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz in /usr/local/lib/python3.5/dist-packages\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.5/dist-packages/en_core_web_lg -->\n",
      "    /usr/local/lib/python3.5/dist-packages/spacy/data/en_core_web_lg\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support for english\n",
    "!sudo python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with Spacy\n",
    "\n",
    "We first import the library and create an `nlp` variable, instantiated for English (`'en'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the space library, instantiated for English\n",
    "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "nlp = spacy.load('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://nicschrading.com/project/Intro-to-NLP-with-spaCy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"There is an art, it says, or rather, a knack to flying. \n",
    "The knack lies in learning how to throw yourself at the ground and miss.\n",
    "In the beginning the Universe was created. This has made a lot of people\n",
    "very angry and been widely regarded as a bad move.\n",
    "This Prof. Panos, Ph.D. costs $12,345.67\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all you have to do to parse text is this:\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[There, is, an, art, ,, it, says, ,, or, rather]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in doc]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the tokens\n",
    "# All you have to do is iterate through the doc\n",
    "# Each token is an object with lots of different properties\n",
    "# A property with an underscore at the end returns the string representation\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# The probability estimate is based on counts from a 3 billion word corpus\n",
    "for i, token in enumerate(doc):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"part of speech:\", token.pos_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get some data\n",
    "\n",
    "First let's get a few text files, so that we can run our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!curl -L 'https://raw.githubusercontent.com/cytora/pycon-nlp-in-10-lines/master/data/article.txt' -o data/article.txt\n",
    "!curl -L 'https://raw.githubusercontent.com/cytora/pycon-nlp-in-10-lines/master/data/pride_and_prejudice.txt' -o data/pride_and_prejudice.txt\n",
    "!curl -L 'https://raw.githubusercontent.com/cytora/pycon-nlp-in-10-lines/master/data/rand-terrorism-dataset.txt'  -o data/rand-terrorism-dataset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read the text file and then we will use the `nlp` object from spacy to analyze the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/article.txt\"\n",
    "text = open(filename, 'r').read()\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tokens, one token per line\n",
    "# The enumerate function is just used to add a counter\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Print Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 sentences (one sentence per line)\n",
    "# The enumerate function is just used to add a counter\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    print(i, \"==>\", sent)\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set([ent.lemma_ for ent in doc.ents])\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_with_type = set([ent.lemma_+\" # \"+ent.label_ for ent in doc.ents ])\n",
    "entities_with_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations = set([ent.lemma_+\" # \"+ent.label_ for ent in doc.ents if ent.label_=='ORG' ])\n",
    "organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [chunk.lemma_ for chunk in doc.noun_chunks if chunk.lemma_ not in entities]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "keywords = Counter()\n",
    "for chunk in chunks:\n",
    "    # print(chunk, nlp.vocab[chunk].prob )\n",
    "    if nlp.vocab[chunk].prob < -8: # probablity value -8 is arbitrarily selected threshold\n",
    "        keywords[chunk] += 1\n",
    "\n",
    "keywords.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent1 in doc.ents:\n",
    "    for ent2 in doc.ents:\n",
    "        similarity = ent1.similarity(ent2)\n",
    "        if similarity > 0.5:\n",
    "            print('{} - {} - {}' .format(ent1, ent2, similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embeddings Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# Let's see if it can figure out this analogy\n",
    "# B is to A as C is to ???\n",
    "a = nlp.vocab['London']\n",
    "b = nlp.vocab['UK']\n",
    "c = nlp.vocab['France']\n",
    "\n",
    "# a = nlp.vocab['Knicks']\n",
    "# b = nlp.vocab['New York']\n",
    "# c = nlp.vocab['Boston']\n",
    "\n",
    "result = a.vector - b.vector + c.vector\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in nlp.vocab if w.has_vector and w.is_title and w.lower_ not in set({a.lower_,b.lower_,c.lower_})})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
