{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Crawling HTML Pages",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/dealing_with_data/blob/master/04-Web_Scraping/B-Crawling_HTML_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGMIH8iiQkaU"
      },
      "source": [
        "# Crawling and Extracting Data from Websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYNQSVkqQkaV"
      },
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNLZmq2gQkag"
      },
      "source": [
        "## Searching in HTML: Fetching the webpage title from ESPN.com\n",
        "\n",
        "Let's start by trying to fetch the headlines from the site ESPN.com.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCx_Tvm3Qkah"
      },
      "source": [
        "import requests # This command allows us to fetch URLs\n",
        "import pandas # To create a dataframe\n",
        "\n",
        "# Let's start by fetching the page, and parsing it\n",
        "url = \"http://www.espn.com/\"\n",
        "\n",
        "# Add a user-agent, to pretend to be a browser, not a Python script\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# get the html of that url\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Parse the web page\n",
        "espn_soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3szL2NDkQkap"
      },
      "source": [
        "Let's start by getting the content of the `<title>` node from the site:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70sDo6dQkaz"
      },
      "source": [
        "results = espn_soup.find('title')\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zude0Ys5Qkap"
      },
      "source": [
        "# Now let's get the text of that node\n",
        "results = espn_soup.find('title').string\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVUdtBuVTcUv"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "* Connect to the NYU Stern website, and fetch the title of the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e4dq_o-Ti7U"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBSnvmd0Qka-",
        "cellView": "form"
      },
      "source": [
        "# @title Solution\n",
        "stern_url = 'http://www.stern.nyu.edu'\n",
        "stern_html = requests.get(stern_url).text\n",
        "stern_soup = BeautifulSoup(stern_html, 'html.parser')\n",
        "\n",
        "title = stern_soup.find('title').string\n",
        "title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p6aTxluTnlS"
      },
      "source": [
        "## Searching for elements of interest in the web page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNzG7ZY9QkbH"
      },
      "source": [
        "Now, let's say that we are looking to retrieve *multiple* elements from a web page. For that we can use the `soup.find_all` command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTvqXmNQkbI"
      },
      "source": [
        "For example, to find all the `<a ...> ... </a>` tags in the returned html, which store the links in the page, we issue the command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-G5eLsCQkbM"
      },
      "source": [
        "# Get all the <a ...> ... </a> elements, which are the links on the page\n",
        "links = espn_soup.find_all(\"a\")\n",
        "len(links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzNj3eZ4QkbY"
      },
      "source": [
        "# Let's pick now one of the many links\n",
        "lnk = links[80]\n",
        "type(lnk.string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhwPXAb1Qkbe"
      },
      "source": [
        " To get parts of the html element that we need, we can use the `get` method (e.g., to get the `href` attribute) and the `text` method (to get the text within the `<a>...</a>` tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvhzBi6cQkbf"
      },
      "source": [
        "lnk.get(\"href\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSPOFRDyQkbl"
      },
      "source": [
        "lnk.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EbdACF-Qkbp"
      },
      "source": [
        "# The strip() removes blank spaces before and after the text\n",
        "lnk.text.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put everything together"
      ],
      "metadata": {
        "id": "eO2mcITht8gx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaj4BbALQkbS"
      },
      "source": [
        "links = espn_soup.find_all(\"a\")\n",
        "\n",
        "# Iterates over all the links (this means all the nodes\n",
        "# that matched the //a XPath query) and prints the content\n",
        "# of the attribute href and the text for that node\n",
        "for link in links:\n",
        "    print(\"==================================\")\n",
        "    print(link.get(\"href\"), \"==>\", link.text.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ4eRqTBQkbr"
      },
      "source": [
        "Now, let's revisit the _list comprehension_ approach that we discussed in the Python Primer session, for quickly constructing lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Vz3APqQkbs"
      },
      "source": [
        "urls = [lnk.get(\"href\") for lnk in espn_soup.find_all('a')]\n",
        "urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can safely skip the code below.\n",
        "# A bit fancier, adding a prefix of http://www.espn.com/ when the URL is\n",
        "# relative and does not include the domain\n",
        "domain = \"http://www.espn.com/\"\n",
        "urls = [\n",
        "    lnk.get(\"href\") if lnk.get(\"href\").startswith(\"http\") else domain + lnk.get(\"href\")\n",
        "    for lnk in espn_soup.find_all('a') if lnk.get(\"href\")\n",
        "]\n",
        "urls"
      ],
      "metadata": {
        "id": "lYpCdc5XOf4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNgg6P7HQkbv"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use a list compresension approach, to get the text_content of all the URLs in the page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0K4iQ82Qkbv"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv8k8jCUQkby"
      },
      "source": [
        "And now create a list where we put together text content and the URL for each link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYN_XbBPRTiy"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhJIYxiDRU2f"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYw-F6zPRSzN"
      },
      "source": [
        "text = [lnk.text.strip() for lnk in espn_soup.find_all(\"a\")]\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not include empty pieces of text\n",
        "text = [lnk.text.strip() for lnk in espn_soup.find_all(\"a\") if len(lnk.text.strip())>0]\n",
        "text"
      ],
      "metadata": {
        "id": "Vi73A_GHPEl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnRT3ecxQkbz"
      },
      "source": [
        "# Creating a list of tuples where we put together href and text for each link\n",
        "list_tuples = [(lnk.get(\"href\"), lnk.text.strip()) for lnk in espn_soup.find_all(\"a\")]\n",
        "list_tuples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGMfS0tHQkb1"
      },
      "source": [
        "# Creating a list of dictionaries with the text and URL for each link\n",
        "list_dicts = [{\"URL\": lnk.get(\"href\"), \"Text\": lnk.text.strip()} for lnk in espn_soup.find_all(\"a\")]\n",
        "list_dicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7VO7birqQkb4"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(list_dicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eyJ4fu-Qkb7"
      },
      "source": [
        "### More Advanced Example: Get the list of headlines from ESPN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTPkHGnQkb7"
      },
      "source": [
        "Now, let's examine how we can get the data from the website. The key is to understand the structure of the HTML, where the data that we need is stored, and how to fetch the elements. Then, using the appropriate XPath queries, we will get what we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_recHuQkb9"
      },
      "source": [
        "Let's start by fetching the page, and parsing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueC9w3oAQkb-"
      },
      "source": [
        "# Let's start by fetching the page, and parsing it\n",
        "url = \"http://www.espn.com/\"\n",
        "\n",
        "# Add a user-agent, to pretend to be a browser, not a Python script\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# get the html of that url\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Parse the web page\n",
        "espn_soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H9-IZeEQkcB"
      },
      "source": [
        "By using the `\"Right-Click > Inspect\"` option of Chrome,\n",
        "we right click on the headlines and select `\"Inspect\"`.\n",
        "This opens the source code.\n",
        "There we see that all under a `<div class=\"headlineStack\">` tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEd6g0HJQkcI"
      },
      "source": [
        "headlineNode = espn_soup.find_all('div', class_='headlineStack')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YE3ytU0QkcK"
      },
      "source": [
        "The result of that operation is a list with 8 elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrhr3KS0QkcL"
      },
      "source": [
        "type(headlineNode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJSB5D2VQkcO"
      },
      "source": [
        "len(headlineNode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq1X3MtdQkcT"
      },
      "source": [
        "Each headline is under a  `<li><a href=\"....\"></a>` tag.\n",
        "So, we get all the `<li><a ...>` tags within the `<div class=\"headlineStack\">`\n",
        "(which is stored in the \"`headlineNode`\" variable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-n8Ajb5QkcU"
      },
      "source": [
        "headlines = headlineNode[1].find_all('li')\n",
        "headlines = [li.find('a') for li in headlines]\n",
        "len(headlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4StKUBexQkcX"
      },
      "source": [
        "Now, we have the nodes with the conent in the headlines variable.\n",
        "We extract the text and the URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ALKiqXwQkcX"
      },
      "source": [
        "data = [{\"Title\": a.text, \"URL\": a.get(\"href\")} for a in headlines]\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRzf_DJMQkca"
      },
      "source": [
        "And let's create our dataframe, so that we can have a better view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bki_w9yoQkca"
      },
      "source": [
        "dataframe = pandas.DataFrame(data)\n",
        "dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbWeA2wCQkce"
      },
      "source": [
        "#### Of course, there are always more than one way to skin a cat...\n",
        "\n",
        "Alternatively, if we did not want to restrict ourselves to just the first headline box, we could write an alternative query, to get back all the headlines, that appear with the pattern of appearing under a `<div class=headlineStack>` and then under a `<li>` tag and then under an `<a>` tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxxXLHsIQkcf"
      },
      "source": [
        "headlines = espn_soup.select('div.headlineStack li a')\n",
        "data = [{\"Title\": a.text, \"URL\": a.get(\"href\")} for a in headlines if a.has_attr('href')]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqrHkZwQQkci"
      },
      "source": [
        "headlines = espn_soup.find_all('a', {'data-mptype': 'headline'})\n",
        "data = [{\"Title\": a.text, \"URL\": a.get(\"href\")} for a in headlines]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3SgwTKOQkck"
      },
      "source": [
        "## In Class Example: Crawl BuzzFeed\n",
        "\n",
        "* We will try to get the top articles that appear on Buzzfeed\n",
        "* We will grab the link for the article, the text of the title,  and the editor.\n",
        "* The results will be stored in a dataframe (we will see in detail what a dataframe is, in a couple of modules)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaOYRZpEQkcm"
      },
      "source": [
        "#your code here\n",
        "import requests\n",
        "\n",
        "resp = requests.get(\"http://www.buzzfeed.com\")\n",
        "buzzfeed = BeautifulSoup(resp.text, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nei56OAhSfcl"
      },
      "source": [
        "story_nodes = buzzfeed.find_all('li', {'aria-label': 'item', \"role\": \"group\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Xwb94eTEiH"
      },
      "source": [
        "len(story_nodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9eDO7QjTEvK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKr3oHlGQkcv",
        "cellView": "form"
      },
      "source": [
        "# @title Solution for Buzzfeed (as of October 23, 2023)\n",
        "\n",
        "import requests # This command allows us to fetch URLs\n",
        "import pandas\n",
        "import re\n",
        "\n",
        "# Let's start by fetching the page, and parsing it\n",
        "\n",
        "resp = requests.get(\"http://www.buzzfeed.com\")\n",
        "buzzfeed = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "articleNodes = buzzfeed.find_all('li', {'aria-label': 'item', \"role\": \"group\"})\n",
        "\n",
        "def parseArticleNode(article):\n",
        "    headline = article.find(\"h2\").text.strip()\n",
        "    headline_link = article.find(\"a\").get(\"href\")\n",
        "\n",
        "    editor_node = article.find(\"div\", {\"class\": \"xs-text-6 text-gray xs-mt1\"})\n",
        "    editor_text = editor_node.text if editor_node else \"\"\n",
        "\n",
        "    regex = re.compile(r'^by (.*)$')\n",
        "    matches = list(regex.finditer(editor_text))\n",
        "    editor = matches[0].group(1) if len(matches)>0 else \"\"\n",
        "\n",
        "    result = {\n",
        "        \"headline\": headline,\n",
        "        \"URL\" : headline_link,\n",
        "        \"editor\" : editor\n",
        "    }\n",
        "    return result\n",
        "\n",
        "data = [parseArticleNode(article) for article in articleNodes]\n",
        "df = pandas.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}